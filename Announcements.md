# Final Projects 

1. **Apimuk Sornsaeng** and **Tanawat Ngampattrapan**, [*Convolutional Neural Networks for Transmission Line Model of Human Cochlear*](https://drive.google.com/file/d/12Wl57WDbpD_IDCC-xlPmvseYfNHB-e5i/view?usp=sharing)

2. **Chanasorn Kongprachaya**, [*CMB Anisotropy maps Simulations using Deep Learning*](https://drive.google.com/file/d/17SK0R8M6p0IQAqfqSXN8mZbrNvStDw1-/view?usp=sharing)

3. **Chayanon Atthapak**, [*Ternary Compound Structures Prediction by Generative Adversarial Networks*](https://drive.google.com/file/d/13WmcKGK61_di-2OYKvtHSIkRaugxXFEM/view?usp=sharing)

4. **Chawakorn Maneerat**, [*A Simple Model to Understand Generalization in Quantum Machine Learning*](https://drive.google.com/file/d/1HKITTNjlfY6bp9WsiG0uA9fqdMAbGgRz/view?usp=sharing)

5. **Pawat Akarapipattana**, [*Learning Geometric Graphs with Restricted Boltzmann Machine*](https://drive.google.com/file/d/16xFOWnLo79p0ZTzzwaJLxEzNTOCCc1HL/view?usp=sharing)

6. **Poramet Pathumsoot, Eakasit**, and **Yaninee Jungjarassub**, [*Approximate Quantum Encoder in Quantum Machine Learning Classification*](https://drive.google.com/file/d/1KXAjTK95f7k72vUAEbdDhkav523N7Zju/view?usp=sharing)

7. **Saksilpa Srisukson**, [*ML in Particle Physics:
An Introduction to Event Classification*](https://drive.google.com/file/d/1agAItOyNLttDQ2_quZfIK2jr230RBvi3/view?usp=sharing)

8. **Wirawat Kokaew** [*Optimal Controls for Open Quantum Systems Based on Reinforcement Learning*](https://drive.google.com/file/d/1oBsethFvHw0knBWAvQREDijYPT5FDh5X/view?usp=sharing)

For more information, see [this link](https://drive.google.com/file/d/12Wl57WDbpD_IDCC-xlPmvseYfNHB-e5i/view?usp=sharing) to Google Drive containing videos, presentation slides, and supplementary materials.


# Announcement

* [03-30-2021] Homework 5 is also uploaded. Since the homework is slightly behind schedule, we encourage you to work on both Homework 4 and 5 together (after you finish 4 of course). It'll be due on April 10 by 10am here https://www.dropbox.com/request/ubBdgC5ovkduWTJMusJu.

* [03-29-2021] Homework 4 is uploaded. You'll get to build your first deep networks from scratch! It's due on April 7 by 10 am here https://www.dropbox.com/request/kYBBOEogbWiKLgDXF2vt.
See the submission instruction in our facebook group.

* [03-16-2021] Homework 3 is uploaded. You'll get to build your first neural nets from scratch! The submission is due by March 25 at noon. You can upload your solutions in the following dropbox link https://www.dropbox.com/request/dSL6mPAm0NQ8KPELMDo9. 

* [03-08-2021] From the outlook and feedback from the course, there'll be no mid-term nor final exam. However, homework will constitute 60% of your grade whereas the final project will constitute 40%. Please spend time thinking and crafting your final research project. We'll be meeting physically to work on your final project proposal together on Wednesday, March 17, from 4.30pm - 6.30pm at Chula Intelligent and Complex Systems new office in https://www.tusparkwha.com/en. Please prepare 5-10 minutes presentation of your final project proposal, then we'll give you feedback. Feel free to refer to additional readings page for your final project ideas. See you soon!

* [02-11-2021] There's no class on Friday, due to วันหยุดราชการ. So I upload one video for this week for you to finish on Kernel methods, which are extremely useful pre-deep learning machine learning models and are precursors to multi-layer perceptrons (baby deep learning). However, we won't have much time to dig deeper, though you'll get to play around in homework 3. Because there are many topics to cover next, we have to stick with the schedule and try to give you useful, practical knowledge to help you build a better ML model. So I encourage you to spend your spare time learning and reading Ensemble Learning, which is how Machine Learning engineers build models in practice with simple ideas that win many Kaggle's competitions, including the Netflix Prize. 
Ensemble Learning is another paradigm of learning based on the fact that many predictors are better than a single predictor, similar to the idea of "wisdom of crowds" permeating social phenomena. The video lecture from the University of Waterloo on Ensemble Learning is really nice, and since it's mostly based on intuitive ideas to combine multiple models (not much theoretical ideas behind it), that video alone should help you understand Mehta's section 8 on combining model, which can give you a really good idea how to improve your model's predictive power in practice (https://www.youtube.com/watch?v=gTUigPt8fVo)
See you next week when we'll cover how to analyze a few Machine Learning problems, using methods of statistical physics.

* [02-09-2021] **Homework 2** is uploaded. Please submit your single zip file "yourname_hw2.zip" that combines ipynb, pdf, pictures, handwriting solutions, or other files that you need here https://www.dropbox.com/request/BWE0rIvQT0KaIBXCoZrm . The online submission will be available until 12.01pm Wednesday 17 Feb. Late submission will receive ε credit.

* [02-03-2021] **Homework 1 Submission Guideline**: Please submit your single zip file "yourname_hw1.zip" that combines ipynb, pdf, pictures, handwriting solutions, or other files that you need here https://www.dropbox.com/request/RU3vDDYVLLac5MyX9sbn . The online submission will be available until 12.01pm Wednesday 3 Feb. Late submission will receive ε credit.

* [02-01-2021] In video 5, we've touched upon Perceptron model for binary classification, without showing the proof of convergence to the empirically optimal hypothesis that can successfully classify two linearly separable dataset. Since Perceptron Learning Algorithm (PLA) is the first algorithm known to *Homo Sapiens* that automatically performs binary classification of linearly separable dataset, it might be worth revisiting the idea behind the first "*Digital Brain*". PLA is also the first example of learning algorithm in this course that requires iterative process to search for an optimal parameter (remember that in linear regularization, we obtain the optimal parameters purely from linear algebra, without needing to perform iterative update.) Typically, theoretical guarantees of the convergence of iterative learning dynamics is rare. If you have time, you're encouraged to read [(this quite nicely written article)](https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975) that sketches the proof of PLA convergence to an empirically optimal hypothesis, where the two classes of data are all correctly labeled. 

* [01-28-2021] Video lecture 3 and 4, as well as accompanying slides, are uploaded. 

* [01-25-2021] Homework 1 is posted [due Wednesday February 3 at Noon]! The total score is 60 points + 20 extra credits (challenging problems). In addition, those who wrote down the best solution to each problem or exercise will receive an extra 5 points! After watching video lecture 3 on regularized linear model you should be able to do homework 1.  Late submission will receive 40% penalty. For theoretical problems/exercises, feel free to write down the solution and attach the pdf files of your write-up and submit together with your ipynb file in a compressed zip file. 

* [01-21-2021] Video lecture 2 is shared [here](https://drive.google.com/drive/folders/1urRjPvKjLZU3QgEDolsQIoC2gssWHB3j?usp=sharing), and the workshop lecture is kindly prepared by Krittin [here](https://drive.google.com/drive/folders/1D72xllKe4zZxsA72R7srdMr6NTIt2xgx?usp=sharing). Please finish all these videos and feel free to post your questions to discuss on Monday on facebook page! 

* [01-19-2021] Video lecture 0 and 1 are shared on [Google Drive](https://drive.google.com/drive/folders/1urRjPvKjLZU3QgEDolsQIoC2gssWHB3j?usp=sharing). The accompanying slides are uploaded on GitHub [here](https://github.com/TChotibut/ml-for-physical-scientists/tree/main/Lecture%20Notes). Statistical Learning Theory is deep and sophisticated, please spend time thinking about concepts in lecture 1. A mathematically oriented student might also appreciate the formulation of supervised learning in the classic paper [F. Cucker and S. Smale, On the mathematical foundations of learning, Bulletin of the American Mathematical Society, 2002.](https://github.com/TChotibut/ml-for-physical-scientists/blob/main/Reading%20Materials/Week1_Cucker_Smale_Mathematical%20Foundations%20of%20Learning.pdf)

* [01-17-2021] Readings for week 1-2: Mehta's section 1-8.  **Preliminary homework 0** [here](https://github.com/sinonkt/ml-for-physical-scientists/blob/main/Homework/HW0_ML%20can%20be%20difficult.ipynb) (due Friday January 22) requires you to install Jupyter notebook and relevant packages such as sklearn on your local computer.  You'll get your hands dirty and get intuition why Machine Learning can be difficult. 

* [01-14-2021] Unless announced here or on the [facebook group](https://www.facebook.com/groups/1033694817095022), we will virtually meet every Monday from 11.00am-noon on [Zoom](https://chula.zoom.us/j/5943943895?pwd=dmpxc3NBMXFPam1FeGtTY2tsdm95UT09) ( Password: MLPhys ). In addition to the weekly virtual meeting, attendees are expected to learn privately at their convenience from the videos to be posted in the announcement ( 2-hour weekly lectures ). The purpose of the virtual meeting is to stimulate the discussion among attendees, as well as to have an interactive session in which questions or comments about lectures and problem sets shall be discussed.
