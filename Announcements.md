# Announcement
* [02-11-2021] There's no class on Friday, due to วันหยุดราชการ. So I upload one video for this week for you to finish on Kernel methods, which are extremely useful pre-deep learning machine learning models and are precursors to multi-layer perceptrons (baby deep learning). However, we won't have much time to dig deeper, though you'll get to play around in homework 3. 

Because there are many topics to cover next, we have to stick with the schedule and try to give you useful, practical knowledge to help you build a better ML model. So I encourage you to spend your spare time learning and reading Ensemble Learning, which is how Machine Learning engineers build models in practice with simple ideas that win many Kaggle's competitions, including the Netflix Prize. 

  Ensemble Learning is another paradigm of learning based on the fact that many predictors are better than a single predictor, similar to the idea of "wisdom of crowds" permeating social phenomena. The video lecture from the University of Waterloo on Ensemble Learning is really nice, and since it's mostly based on intuitive ideas to combine multiple models (not much theoretical ideas behind it), that video alone should help you understand Mehta's section 8 on combining model, which can give you a really good idea how to improve your model's predictive power in practice (https://www.youtube.com/watch?v=gTUigPt8fVo)
.
See you next week when we'll cover how to analyze a few Machine Learning problems, using methods of statistical physics.

* [02-09-2021] **Homework 2** is uploaded. Please submit your single zip file "yourname_hw2.zip" that combines ipynb, pdf, pictures, handwriting solutions, or other files that you need here https://www.dropbox.com/request/BWE0rIvQT0KaIBXCoZrm . The online submission will be available until 12.01pm Wednesday 17 Feb. Late submission will receive ε credit.

* [02-03-2021] **Homework 1 Submission Guideline**: Please submit your single zip file "yourname_hw1.zip" that combines ipynb, pdf, pictures, handwriting solutions, or other files that you need here https://www.dropbox.com/request/RU3vDDYVLLac5MyX9sbn . The online submission will be available until 12.01pm Wednesday 3 Feb. Late submission will receive ε credit.

* [02-01-2021] In video 5, we've touched upon Perceptron model for binary classification, without showing the proof of convergence to the empirically optimal hypothesis that can successfully classify two linearly separable dataset. Since Perceptron Learning Algorithm (PLA) is the first algorithm known to *Homo Sapiens* that automatically performs binary classification of linearly separable dataset, it might be worth revisiting the idea behind the first "*Digital Brain*". PLA is also the first example of learning algorithm in this course that requires iterative process to search for an optimal parameter (remember that in linear regularization, we obtain the optimal parameters purely from linear algebra, without needing to perform iterative update.) Typically, theoretical guarantees of the convergence of iterative learning dynamics is rare. If you have time, you're encouraged to read [(this quite nicely written article)](https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975) that sketches the proof of PLA convergence to an empirically optimal hypothesis, where the two classes of data are all correctly labeled. 

* [01-28-2021] Video lecture 3 and 4, as well as accompanying slides, are uploaded. 

* [01-25-2021] Homework 1 is posted [due Wednesday February 3 at Noon]! The total score is 60 points + 20 extra credits (challenging problems). In addition, those who wrote down the best solution to each problem or exercise will receive an extra 5 points! After watching video lecture 3 on regularized linear model you should be able to do homework 1.  Late submission will receive 40% penalty. For theoretical problems/exercises, feel free to write down the solution and attach the pdf files of your write-up and submit together with your ipynb file in a compressed zip file. 

* [01-21-2021] Video lecture 2 is shared [here](https://drive.google.com/drive/folders/1urRjPvKjLZU3QgEDolsQIoC2gssWHB3j?usp=sharing), and the workshop lecture is kindly prepared by Krittin [here](https://drive.google.com/drive/folders/1D72xllKe4zZxsA72R7srdMr6NTIt2xgx?usp=sharing). Please finish all these videos and feel free to post your questions to discuss on Monday on facebook page! 

* [01-19-2021] Video lecture 0 and 1 are shared on [Google Drive](https://drive.google.com/drive/folders/1urRjPvKjLZU3QgEDolsQIoC2gssWHB3j?usp=sharing). The accompanying slides are uploaded on GitHub [here](https://github.com/TChotibut/ml-for-physical-scientists/tree/main/Lecture%20Notes). Statistical Learning Theory is deep and sophisticated, please spend time thinking about concepts in lecture 1. A mathematically oriented student might also appreciate the formulation of supervised learning in the classic paper [F. Cucker and S. Smale, On the mathematical foundations of learning, Bulletin of the American Mathematical Society, 2002.](https://github.com/TChotibut/ml-for-physical-scientists/blob/main/Reading%20Materials/Week1_Cucker_Smale_Mathematical%20Foundations%20of%20Learning.pdf)

* [01-17-2021] Readings for week 1-2: Mehta's section 1-8.  **Preliminary homework 0** [here](https://github.com/sinonkt/ml-for-physical-scientists/blob/main/Homework/HW0_ML%20can%20be%20difficult.ipynb) (due Friday January 22) requires you to install Jupyter notebook and relevant packages such as sklearn on your local computer.  You'll get your hands dirty and get intuition why Machine Learning can be difficult. 

* [01-14-2021] Unless announced here or on the [facebook group](https://www.facebook.com/groups/1033694817095022), we will virtually meet every Monday from 11.00am-noon on [Zoom](https://chula.zoom.us/j/5943943895?pwd=dmpxc3NBMXFPam1FeGtTY2tsdm95UT09) ( Password: MLPhys ). In addition to the weekly virtual meeting, attendees are expected to learn privately at their convenience from the videos to be posted in the announcement ( 2-hour weekly lectures ). The purpose of the virtual meeting is to stimulate the discussion among attendees, as well as to have an interactive session in which questions or comments about lectures and problem sets shall be discussed.
