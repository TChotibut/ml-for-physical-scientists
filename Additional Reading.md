# A List of Recent Work/Talks

This is a compilation of recent work that might be a source of inspiration for your final project.

## Classical Machine Learning

### Papers

* Ruehle, F. **Data Science Applications to String Theory**, [(Physics Report 2020)](https://www.sciencedirect.com/science/article/pii/S0370157319303072)

* Abbara, A. et. al. **Rademacher complexity and spin glasses: A link between the replica and statistical theories of learning**, ([Proceedings of Machine Learning Research 2020](https://cloud.math.princeton.edu/index.php/s/zakSjPtSxPqw62A)).

* Lesieur, T. et. al. **Statistical and computational phase transitions in spiked tensor estimation** ([ArXiv 2017](https://arxiv.org/abs/1701.08010))

* Cakmak, B., and Opper, M., **A Dynamical Mean-Field Theory for Learning in Restricted Boltzmann Machines** ([Arxiv 2020](https://arxiv.org/pdf/2005.01560.pdf))


## Modern Machine Learning (Deep Learning)

### Papers
* Vasudevan, et. al. **Off-the-shelf deep learning is not enough, and requires parsimony, Bayesianity, and causality**, [(npj: computational materials 2021)](https://www.nature.com/articles/s41524-020-00487-0?fbclid=IwAR1rW_RQPSkekx6Rvtik184RYuiwGGBvYpBCZCUdsWSxIFvu3_RtaQoLe74)

* Belkin, M. et. al. **Reconciling modern machine learning practice and the bias-variance trade-off** (PNAS 2019, [ArXiv](https://arxiv.org/abs/1812.11118), see also the ([history of the double descent](https://www.pnas.org/content/117/20/10625)))

* Jacot, A. et. al. **Neural Tangent Kernel: Convergence and Generalization in Neural Networks** ([NeurIPS 2018](https://proceedings.neurips.cc/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf))

* Lee, J. et. al. **Deep Neural Networks as Gaussian Process** ([ICLR 2018](https://openreview.net/pdf?id=B1EA-M-0Z))

* Yaida, Sho. **Non-Gaussian processes and neural networks at finite widths** ([Proceedings of Machine Learning Research 2020](https://cloud.math.princeton.edu/index.php/s/PF8g28ctpEcp3Ne))

* Harutyunyan, H. et. al. **Improving Generalization by Controlling Label-Noise Information in Neural Network Weights** ([ICML 2020](https://arxiv.org/pdf/2002.07933.pdf))
* 
### Talks

* Yasaman Bahri, **Deep Learning: Theoretical Building Blocks From The Study of Wide Networks** ([YouTube](https://www.youtube.com/watch?v=xIo5er6jR9U&feature=youtu.be))

## Quantum Machine Learning

* Bharti, K., et. al. **Noisy Intermediate-scale quantum (NISQ) algorithms**, [(ArXiv 2021)](https://arxiv.org/pdf/2101.08448.pdf)

* Schuld, M., **Quantum machine learning models are kernel methods**, [(ArXiv 2021)](https://arxiv.org/abs/2101.11020)

* Beer, K., et. al., **Training deep quantum neural networks**, [(Nature Communications 2020)](https://www.nature.com/articles/s41467-020-14454-2)

* Abbas, A., et. al. **The power of quantum neural networks**, [(ArXiv 2020)](https://arxiv.org/abs/2011.00027)

* Austen, L. **Quantum Ground States from Reinforcement Learning** ([Proceedings of Machine Learning Research 2020](https://cloud.math.princeton.edu/index.php/s/o2qtK9f3GtYDQ9L))

* Stoudenmire, E., Schwab, D. J. **Supervised Learning with Tensor Networks** ([NeurIPS 2016](https://papers.nips.cc/paper/2016/hash/5314b9674c86e3f9d1ba25ef9bb32895-Abstract.html))

* Banchi, L. et. al. **Generalization in Quantum Machine Learning: a Quantum Information Perspective** ([ArXiv 2021](https://arxiv.org/pdf/2102.08991.pdf))
