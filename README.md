# Machine Learning for Physical Scientists
[ [Local](http://localhost:8888/git-pull?repo=https%3A%2F%2Fgithub.com%2FTChotibut%2Fml-for-physical-scientists&urlpath=lab%2Fworkspaces%2Fauto-8%2Ftree%2Fml-for-physical-scientists%2FREADME.md&branch=main) | CHICS hub ]

Special Topic in Solid State Physics: Machine Learning for Physical Scientists

รหัสวิชา 2304641 ปีการศึกษา 2563

## Schedule:

| Week     |     Topics    |  Readings | Homework  |
|----------|:-------------:|------:|------:|
| Week 0-1 |  Course Introduction ([video](https://drive.google.com/file/d/1RAJHJoJSCfMkqmy3aJAucdJ_hIlxQ7Xa/view?usp=sharing), [slides](https://github.com/TChotibut/ml-for-physical-scientists/blob/main/Lecture%20Notes/Lecture_0_Course%20Introduction.pdf)), Introduction to Statistical Learning Theory Framework ([video](https://drive.google.com/file/d/1UfiHOuo-aLQF88yGWunhs-k2KnngXEKV/view?usp=sharing), [slides](https://github.com/TChotibut/ml-for-physical-scientists/blob/main/Lecture%20Notes/Lecture_1_IntroStatisticalLearning.pdf))  | Mehta's Sec. 1-3, Cucker and Smale's ([paper](https://github.com/TChotibut/ml-for-physical-scientists/blob/main/Reading%20Materials/Week1_Cucker_Smale_Mathematical%20Foundations%20of%20Learning.pdf))   | HW 0|
| Week 2 |   Linear Regression ([video](https://drive.google.com/file/d/10_tHnX0cVekjks3l5TyVzKdYoDFmG3Y9/view?usp=sharing), [slides](https://github.com/TChotibut/ml-for-physical-scientists/blob/main/Lecture%20Notes/Lecture_2_The%20Simplest%20Supervised%20Learning.pdf)), Regularization ([video](https://drive.google.com/file/d/1i1TOCqDouCUH-s8PbRaMlWNOAL5MekkL/view?usp=sharing),[slides](https://github.com/TChotibut/ml-for-physical-scientists/blob/main/Lecture%20Notes/Lecture_3_Regularization.pdf))   | Mehta's Sec. 5-6   | HW 1 (out) |
| Week 3 | Bias-Variance Decomposition, Introduction to Bayesian Inference ([video](https://drive.google.com/file/d/1E6LEiF13F3qPzeUbkZnEz3z7UbbN2v9w/view?usp=sharing),[slides](https://github.com/TChotibut/ml-for-physical-scientists/blob/main/Lecture%20Notes/Lecture_4_BiasVariance_IntroBayesian.pdf)), MLE MAP and Introduction to multi-class Classification ([video](https://drive.google.com/file/d/1fD2FbDo0fmvnhYmxKVm8BOqmi2Xnur99/view?usp=sharing),[slides](https://github.com/TChotibut/ml-for-physical-scientists/blob/main/Lecture%20Notes/Lecture_5_ML_MAP_IntroSupervisedClassification.pdf)), Logistic Regression ([video](https://drive.google.com/file/d/10iVyldJL1Cyz7Kp242APbvh7X7r2pi2L/view?usp=sharing),[slides](https://github.com/TChotibut/ml-for-physical-scientists/blob/main/Lecture%20Notes/Lecture_6_Logistic%20Regression.pdf))  |  Mehta's Sec. 5-7, Bias-variance ([note](https://github.com/TChotibut/ml-for-physical-scientists/blob/main/Lecture%20Notes/SM_BiasVarianceDecomposition.pdf)), Perceptron Learning Algortihm ([link](https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975))  | HW 1 (due)|
| Week 4 | Kernel Methods ([video](https://drive.google.com/file/d/14dT_BxxWqQPiA7k_21l7QZ7_0zdj4tY8/view?usp=sharing ),[slides](https://github.com/TChotibut/ml-for-physical-scientists/blob/main/Lecture%20Notes/Lecture_7_Kernel%20Method.pdf)), Bagging (reducing variance) and Boosting (reducing bias) (A great [video](https://www.youtube.com/watch?v=gTUigPt8fVo) lecture from the University of Waterloo) | Mehta's section 8, Shalev-Shwartz's book chapter on kernel methods([link](https://github.com/TChotibut/ml-for-physical-scientists/blob/main/Reading%20Materials/Shalev-Shwartz_Kernel.pdf)), How to win the Netflix prize with ensemble learning ([link](https://static.aminer.org/pdf/PDF/000/294/514/random_decision_forests.pdf))| HW 2 (out) |
| Week 5 | Intro to Phase Transitions ([video 1](https://www.youtube.com/watch?v=rbbyl0SD3TQ), [video 2](https://www.youtube.com/watch?v=dZ9neWVZcy0)), Intro to Statistical Physics of Inference and Spin Glasses ([video](https://www.youtube.com/watch?v=rbbyl0SD3TQ)) | Skim this excellent review paper to explore how phase transitions can arise from statistical inference ([paper](https://github.com/TChotibut/ml-for-physical-scientists/blob/main/Reading%20Materials/Zdeborova_Krzakala_AdvPhys2016.pdf)) | HW 2 (due)|

## Course Description: 
In the age of Big Data, “Artificial Intelligence (AI) is the new electricity” is perhaps not an overstatement. Well-developed AI can turn seemingly useless pile of information into useful knowledge, which in turn can influence important decision making or can even fuel scientific discoveries. In the style that is suitable for physicists, this course provides fundamental concepts and techniques of Machine Learning (ML), a subfield of AI that harnesses the power of computation, whether classical or quantum, to turn data into useful computational models. We will first cover the core principles of statistical learning theory, which is a backbone of ML, including overfitting, regularization, bias-variance tradeoff, generalization, and model complexity. We will then cover important classical models of supervised and unsupervised learning such as ensemble models, Deep Learning, clustering and manifold learning, energy-based models such as Restricted Boltzmann Machines, and variational inference. Throughout the course, we will make an emphasis on natural connections between ML and statistical physics. Also, some quantum-inspired algorithms for ML will be presented towards the end of the course. In addition to pencil and paper style homework, we will also have a python-based or Tensorflow-based programming component of homework. We will use Jupyter notebooks for the programming components, where you will learn how to deploy ML algorithms in practice from physics-inspired datasets such as the Ising Model, the XY Model and topological phase transitions, Monte Carlo simulations for some scattering experiments in LHC, and etc.  We will end the course with project-based presentations, where students will tackle some of open problems in ML that physicists might be able to contribute, or applying ML to solve complex physics problems of interests. 

## Grading Scheme: 
* 50% Homework
* 20% Exam
* 30% Final Project and Presentation

## References: 

1.	David Mackay's [Information Theory, Inference, and Learning Algorithms](http://www.inference.eng.cam.ac.uk/mackay/itila/).
2.	Pankaj Mehta, et. al. [A high-bias, low-variance introduction to Machine Learning for physicists](https://www.sciencedirect.com/science/article/pii/S0370157319300766)
3.	Caltech’s [Learning from Data](https://work.caltech.edu/telecourse.html)
4.	Peter Wittek’s [Quantum Machine Learning: What Quantum Computing Means to Data Mining](https://www.sciencedirect.com/book/9780128009536/quantum-machine-learning)
5.	Roman Orus’ [A Practical Introduction to Tensor Networks: Matrix Product States and Projected Entangled Pair States](https://arxiv.org/pdf/1306.2164.pdf)

## Prerequisites:

1.	Quantum Mechanics 2 ([Chula Playlist](https://www.youtube.com/playlist?list=PL0XuSm2_1reOcN_tPDHlqbflzaRFEhoMC))
2.	Statistical Physics ([Chula Playlist](https://www.youtube.com/playlist?list=PL0XuSm2_1reOH2Zsr0gKNA1uRCJ290eco))
3.	Python programming ([Recommended Course](https://www.coursera.org/learn/python-crash-course))

## Extras, Notebook git puller list:
* [Extra] Petter Wittek, Quantum Machine Learning [ [edX](https://www.edx.org/course/quantum-machine-learning) | [github](https://github.com/qosf/qml-mooc) | [local](http://localhost:8888/git-pull?repo=https%3A%2F%2Fgithub.com%2Fqosf%2Fqml-mooc&urlpath=lab%2Fworkspaces%2Fauto-8%2Ftree%2Fqml-mooc%2FREADME.md&branch=master) | chics hub ]
